{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The main focus of this assignment is clustering from theoretical as well as practical perspective_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Clustering (Manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following dataset, perform the clustering “by hand”:\n",
    "\n",
    "17 28 50 60 80 89 150 167 171 189 \n",
    "1. \tUse the K-means algorithm with K= 3 to cluster the data\n",
    "2. \tUse hierarchical agglomerative clustering with single linkage to cluster the data\n",
    "3. \tUse hierarchical agglomerative clustering with complete linkage to cluster the data\n",
    "4. \tFor K-means What will the final clusters be after 3 iterations if k=3 and the initial centers are 150, 171 and 189"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Do not write code to answer this question_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Clustering (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dataset of accepted papers at the AAAI 2014 conference to find clusters of papers using K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ccc_v1_w_Nzk1_10366\n",
      "[nltk_data]     1/asn40522_6/asn40523_1/work/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load the Relevant libraries\n",
    "import sklearn as sk\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# URL for the AAAI (UW Repository)\n",
    "url = \"AAAI2014AcceptedPapers.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do pre-processing to the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use the abstract field to define papers\n",
    "\n",
    "# remove special characters\n",
    "df['cleaned'] = df['abstract'].apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "\n",
    "# turn to all lower-case\n",
    "df['cleaned'] = df['cleaned'].apply(lambda x: str.lower(x))\n",
    "\n",
    "# tokenize strings and remove stop-words\n",
    "def token_remove_stop(x):\n",
    "    t = nltk.word_tokenize(x)\n",
    "    t = [word for word in t if word not in stop_words]\n",
    "    return t\n",
    "\n",
    "df['cleaned'] = df['cleaned'].apply(lambda x: token_remove_stop(x))\n",
    "\n",
    "# perform stemming\n",
    "def stemmer(x):\n",
    "    return [ps.stem(word) for word in x]\n",
    "df['cleaned'] = df['cleaned'].apply(lambda x: stemmer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 340 ms, sys: 0 ns, total: 340 ms\n",
      "Wall time: 356 ms\n",
      "(398, 83)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',max_df=0.8, max_features=1000, strip_accents='ascii',\n",
    "                                 min_df=0.1, stop_words='english',\n",
    "                                 use_idf=True, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(df['abstract']) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2)\n",
    "km = km.fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1) Vary the number of K from 2 to 6 and show if the results vary and assess the clusters obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2) Make a case regarding which clusters ‘make sense’ e.g., is there a cluster were papers on reinforcement learning are together vs. another cluster which has papers on deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
