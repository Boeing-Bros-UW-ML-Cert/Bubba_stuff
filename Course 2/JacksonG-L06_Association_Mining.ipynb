{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The main focus of this assignment is Frequent Patternset Mining from theoretical as well as practical perspective_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{onion, apple, butter, yogurt, eggs, apple} <br>\n",
    "{onion, apple, butter, yogurt, eggs, milk} <br>\n",
    "{eggs, milk, butter, yogurt, eggs, milk} <br>\n",
    "{cheese, milk, butter, yogurt, eggs, milk} <br>\n",
    "{cheese, milk, eggs, milk}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "Using the FP-Tree algorithm with min support  = 0.6 and min confidence = 0.75 find all 3-frequent and all 4-frequent items. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Calculate support\n",
    "\n",
    "| Word | Count | Suppport|Meets Min Suppport|\n",
    "|---|:---:|---:|:---:|\n",
    "|onion|2|.4| |\n",
    "|apple|3|.6|x|\n",
    "|butter|4|.8|x|\n",
    "|yogurt|4|.8|x|\n",
    "|eggs|5|1.0|x|\n",
    "|milk|7|1.4|x|\n",
    "|cheese|2|.4| |\n",
    "\n",
    "Step 2: Eliminate items that don't meet minimum support and create F-list\n",
    "\n",
    "F-list : {milk, eggs, butter, yogurt, apple}\n",
    "\n",
    "|Curated data set|\n",
    "|---|\n",
    "|{apple, butter, yogurt, eggs}|\n",
    "|{apple, butter, yogurt, eggs, milk}|\n",
    "|{eggs, milk, butter, yogurt}|\n",
    "|{milk, butter, yogurt, eggs}|\n",
    "|{milk, eggs}|\n",
    "\n",
    "Step 3: Calculate confidence for combos, in order of appearance in F-list\n",
    "\n",
    "|Combo|Confidence|Meets Min Confidence|\n",
    "|---|:---:|\n",
    "|{milk,eggs}|.80|x|\n",
    "|{milk,butter}|.60| \n",
    "|{milk,yogurt}|.60|\n",
    "|{milk,apple}|.20|\n",
    "\n",
    "Step 4: Eliminate transactions that do not include our transactions that meet minimum confidence level\n",
    "\n",
    "|Curated data set|\n",
    "|---|\n",
    "|{apple, butter, yogurt, eggs, milk}|\n",
    "|{eggs, milk, butter, yogurt}|\n",
    "|{milk, butter, yogurt, eggs}|\n",
    "|{milk, eggs}|\n",
    "\n",
    "Step 5: Calculate confidence for 3-frequent combos and eliminate combos that don't reach the minimum\n",
    "\n",
    "|3-frequent terms|Confidence|Meets Min Confidence|\n",
    "|---|:---:|\n",
    "|{milk,eggs,butter}|.75|x|\n",
    "|{milk,eggs,yogurt}|.75|x|\n",
    "|{milk,eggs,apple}|.25|\n",
    "\n",
    "|Curated data set|\n",
    "|---|\n",
    "|{apple, butter, yogurt, eggs, milk}|\n",
    "|{eggs, milk, butter, yogurt}|\n",
    "|{milk, butter, yogurt, eggs}|\n",
    "\n",
    "Step 5: Calculate confidence for 4-frequent combos\n",
    "\n",
    "|4-frequent terms|Confidence|Meets Min Confidence|\n",
    "|---|:---:|\n",
    "|{milk,eggs,butter,yogurt}|1.0|x|\n",
    "|{milk,eggs,butter,apple}|.33||\n",
    "\n",
    "##### 3-frequent terms: {milk, eggs, butter}, {milk, eggs, yogurt}\n",
    "##### 4-frequent terms: {milk, eggs, butter, yogurt}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List 3 frequent item rulesets that can be derived from the previous set\n",
    "- Show that any 3-frequent itemsets can be derived from 2-frequent itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Note: Do not write code to answer this question_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 2: Applications of FP Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the NIS Flanders Traffic Accident Dataset apply Frequent Itemset Mining to identify the road conditions, type of roads associated with accidents. Identify other rules of interest that are associated with accidents or lack thereof. Make a case regarding how you can use these rules to reduce traffic accidents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'urllib' has no attribute 'urlopen'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-aa37cb243fb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# download the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'urllib' has no attribute 'urlopen'"
     ]
    }
   ],
   "source": [
    "# Load the Relevant libraries\n",
    "import sklearn as sk\n",
    "import urllib\n",
    "\n",
    "\n",
    "# URL for the Traffic Data (UW Repository)\n",
    "# link for dataset description http://fimi.uantwerpen.be/data/accidents.pdf\n",
    "url = \"NISTraffic.csv\"\n",
    "\n",
    "# download the file\n",
    "raw_data = urllib.urlopen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from sys import stdout\n",
    "\n",
    "\n",
    "class cached_property(object):\n",
    "    \"\"\"A cached property only computed once\n",
    "    \"\"\"\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __get__(self, obj, cls):\n",
    "        if obj is None: return self\n",
    "        value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
    "        return value\n",
    "\n",
    "\n",
    "class Base(object):\n",
    "    \"\"\"A base workflow for Apriori algorithm\n",
    "    \"\"\"\n",
    "    def _before_generate_frequent_itemset(self):\n",
    "        \"\"\"Invoked before generate_frequent_itemset()\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _after_generate_frequent_itemset(self):\n",
    "        \"\"\"Invoked before generate_frequent_itemset()\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def generate_frequent_itemset(self):\n",
    "        \"\"\"Generate and return frequent itemset\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"generate_frequent_itemset(self) need to be implemented.\")\n",
    "\n",
    "    def _before_generate_rule(self):\n",
    "        \"\"\"Invoked before generate_frequent_itemset()\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _after_generate_rule(self):\n",
    "        \"\"\"Invoked before generate_frequent_itemset()\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def generate_rule(self):\n",
    "        \"\"\"Generate and return rule\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"generate_rule(self) need to be implemented.\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run Apriori algorithm and return rules\n",
    "        \"\"\"\n",
    "        # generate frequent itemset\n",
    "        self._before_generate_frequent_itemset()\n",
    "        self.generate_frequent_itemset()\n",
    "        self._after_generate_frequent_itemset()\n",
    "        # generate rule\n",
    "        self._before_generate_rule()\n",
    "        self.generate_rule()\n",
    "        self._after_generate_rule()\n",
    "\n",
    "\n",
    "class Apriori(Base):\n",
    "    \"\"\"A simple implementation of Apriori algorithm\n",
    "        Example:\n",
    "        \n",
    "        dataset = [\n",
    "            ['bread', 'milk'],\n",
    "            ['bread', 'diaper', 'beer', 'egg'],\n",
    "            ['milk', 'diaper', 'beer', 'cola'],\n",
    "            ['bread', 'milk', 'diaper', 'beer'],\n",
    "            ['bread', 'milk', 'diaper', 'cola'],\n",
    "        ]\n",
    "        minsup = minconf = 0.6\n",
    "        apriori = Apriori(dataset, minsup, minconf)\n",
    "        apriori.run()\n",
    "        apriori.print_rule()\n",
    "        Results:\n",
    "            Rules\n",
    "            milk --> bread (confidence = 0.75)\n",
    "            bread --> milk (confidence = 0.75)\n",
    "            diaper --> bread (confidence = 0.75)\n",
    "            bread --> diaper (confidence = 0.75)\n",
    "            beer --> diaper (confidence = 1.0)\n",
    "            diaper --> beer (confidence = 0.75)\n",
    "            diaper --> milk (confidence = 0.75)\n",
    "            milk --> diaper (confidence = 0.75)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transaction_list, minsup, minconf, selected_items=None):\n",
    "        \"\"\"Initialization\n",
    "        :param transaction_list: a list cantains transaction\n",
    "        :param minsup: minimum support\n",
    "        :param minconf: minimum confidence\n",
    "        :param selected_items: selected items in frequent itemset, default `None`\n",
    "        \"\"\"\n",
    "        self.transaction_list = transaction_list\n",
    "        self.transaction_list_full_length = len(transaction_list)\n",
    "        self.minsup = minsup\n",
    "        self.minconf = minconf\n",
    "        if selected_items is not None and selected_items is not []:\n",
    "            self.selected_items = frozenset(selected_items)\n",
    "        else:\n",
    "            self.selected_items = None\n",
    "\n",
    "        self.frequent_itemset = dict()\n",
    "        # support for every frequent itemset\n",
    "        self.frequent_itemset_support = defaultdict(float)\n",
    "        # convert transaction_list\n",
    "        self.transaction_list = list([frozenset(transaction) \\\n",
    "            for transaction in transaction_list])\n",
    "\n",
    "        self.rule = []\n",
    "\n",
    "    def set_selected_items(self, selected_items):\n",
    "        \"\"\"Set selected items\n",
    "        \"\"\"\n",
    "        self.selected_items = frozenset(selected_items)\n",
    "\n",
    "    @cached_property\n",
    "    def items(self):\n",
    "        \"\"\"Return all items in the self.transaction_list\n",
    "        \"\"\"\n",
    "        items = set()\n",
    "        for transaction in self.transaction_list:\n",
    "            for item in transaction:\n",
    "                items.add(item)\n",
    "        return items\n",
    "\n",
    "    def filter_with_minsup(self, itemsets):\n",
    "        \"\"\"Return subset of itemsets which satisfies minsup\n",
    "        and record their support\n",
    "        \"\"\"\n",
    "        local_counter = defaultdict(int)\n",
    "        for itemset in itemsets:\n",
    "            for transaction in self.transaction_list:\n",
    "                if itemset.issubset(transaction):\n",
    "                    local_counter[itemset] += 1\n",
    "        # filter with counter\n",
    "        result = set()\n",
    "        for itemset, count in local_counter.items():\n",
    "            support = float(count) / self.transaction_list_full_length\n",
    "            if support >= self.minsup:\n",
    "                result.add(itemset)\n",
    "                self.frequent_itemset_support[itemset] = support\n",
    "        return result\n",
    "\n",
    "    def _after_generate_frequent_itemset(self):\n",
    "        \"\"\"Filter frequent itemset with selected items\n",
    "        \"\"\"\n",
    "        if self.selected_items is None:\n",
    "            return\n",
    "        local_remove = []\n",
    "        for key, val in self.frequent_itemset.items():\n",
    "            for itemset in val:\n",
    "                if not self.selected_items.issubset(itemset):\n",
    "                    local_remove.append((key, itemset))\n",
    "        for (key, itemset) in local_remove:\n",
    "            self.frequent_itemset[key].remove(itemset)\n",
    "\n",
    "    def generate_frequent_itemset(self):\n",
    "        \"\"\"Generate and return frequent itemset\n",
    "        \"\"\"\n",
    "        def _apriori_gen(itemset, length):\n",
    "            \"\"\"Return candidate itemset with given itemset and length\n",
    "            \"\"\"\n",
    "            # simply use F(k-1) x F(k-1) (itemset + itemset)\n",
    "            return set([x.union(y) for x in itemset for y in itemset \\\n",
    "                if len(x.union(y)) == length])\n",
    "\n",
    "        k = 1\n",
    "        current_itemset = set()\n",
    "        # generate 1-frequnt_itemset\n",
    "        for item in self.items: current_itemset.add(frozenset([item]))\n",
    "        self.frequent_itemset[k] = self.filter_with_minsup(current_itemset)\n",
    "        # generate k-frequent_itemset\n",
    "        while True:\n",
    "            k += 1\n",
    "            current_itemset = _apriori_gen(current_itemset, k)\n",
    "            current_itemset = self.filter_with_minsup(current_itemset)\n",
    "            if current_itemset != set([]):\n",
    "                self.frequent_itemset[k] = current_itemset\n",
    "            else:\n",
    "                break\n",
    "        return self.frequent_itemset\n",
    "\n",
    "    def _generate_rule(self, itemset, frequent_itemset_k):\n",
    "        \"\"\"Generate rule with F(k) in DFS style\n",
    "        \"\"\"\n",
    "        # make sure the itemset has at least two element to generate the rule\n",
    "        if len(itemset) < 2:\n",
    "            return\n",
    "        for element in combinations(list(itemset), 1):\n",
    "            rule_head = itemset - frozenset(element)\n",
    "            confidence = self.frequent_itemset_support[frequent_itemset_k] / \\\n",
    "                self.frequent_itemset_support[rule_head]\n",
    "            if confidence >= self.minconf:\n",
    "                rule = ((rule_head, itemset - rule_head), confidence)\n",
    "                # if rule not in self.rule, add and recall _generate_rule() in DFS\n",
    "                if rule not in self.rule:\n",
    "                    self.rule.append(rule);\n",
    "                    self._generate_rule(rule_head, frequent_itemset_k)\n",
    "\n",
    "    def generate_rule(self):\n",
    "        \"\"\"Generate and return rule\n",
    "        \"\"\"\n",
    "        # generate frequent itemset if not generated\n",
    "        if len(self.frequent_itemset) == 0:\n",
    "            self.generate_frequent_itemset()\n",
    "        # generate in DFS style\n",
    "        for key, val in self.frequent_itemset.items():\n",
    "            if key == 1:\n",
    "                continue\n",
    "            for itemset in val:\n",
    "                self._generate_rule(itemset, itemset)\n",
    "        return self.rule\n",
    "\n",
    "    def print_frequent_itemset(self):\n",
    "        \"\"\"Print out frequent itemset\n",
    "        \"\"\"\n",
    "        stdout.write('======================================================\\n')\n",
    "        stdout.write('Frequent itemset:\\n')\n",
    "        for key, val in self.frequent_itemset.items():\n",
    "            #stdout.write('frequent itemset size of {0}:\\n'.format(key))\n",
    "            for itemset in val:\n",
    "                stdout.write('(')\n",
    "                stdout.write(', '.join(itemset))\n",
    "                stdout.write(')')\n",
    "                stdout.write('  support = {0}\\n'.format(round(self.frequent_itemset_support[itemset], 3)))\n",
    "        stdout.write('======================================================\\n')\n",
    "\n",
    "    def print_rule(self):\n",
    "        \"\"\"Print out rules\n",
    "        \"\"\"\n",
    "        stdout.write('======================================================\\n')\n",
    "        stdout.write('Rules:\\n')\n",
    "        for rule in self.rule:\n",
    "            head = rule[0][0]\n",
    "            tail = rule[0][1]\n",
    "            confidence = rule[1]\n",
    "            stdout.write('(')\n",
    "            stdout.write(', '.join(head))\n",
    "            stdout.write(')')\n",
    "            stdout.write(' ==> ')\n",
    "            stdout.write('(')\n",
    "            stdout.write(', '.join(tail))\n",
    "            stdout.write(')')\n",
    "            stdout.write('  confidence = {0}\\n'.format(round(confidence, 3)))\n",
    "        stdout.write('======================================================\\n')\n",
    "\n",
    "\n",
    "class ImprovedApriori(Apriori):\n",
    "    \"\"\"Use Hash to filter frequent itemsets\n",
    "    \"\"\"\n",
    "\n",
    "    def filter_with_minsup(self, itemsets):\n",
    "        \"\"\"Return subset of itemset which satisfies minsup\n",
    "        and record their support\n",
    "        \"\"\"\n",
    "        for itemset in itemsets:\n",
    "            k = len(itemset)\n",
    "            break\n",
    "        local_counter = defaultdict(int)\n",
    "        for transaction in self.transaction_list:\n",
    "            for itemset in combinations(list(transaction), k):\n",
    "                if frozenset(itemset) in itemsets:\n",
    "                    local_counter[frozenset(itemset)] += 1\n",
    "        # filter with counter\n",
    "        result = set()\n",
    "        for itemset, count in local_counter.items():\n",
    "            support = float(count) / self.transaction_list_full_length\n",
    "            if support >= self.minsup:\n",
    "                result.add(itemset)\n",
    "                self.frequent_itemset_support[itemset] = support\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset1 = [\n",
    "    ['bread', 'milk'],\n",
    "    ['bread', 'diaper', 'beer', 'egg'],\n",
    "    ['milk', 'diaper', 'beer', 'cola'],\n",
    "    ['bread', 'milk', 'diaper', 'beer'],\n",
    "    ['bread', 'milk', 'diaper', 'cola'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = []\n",
    "T = []\n",
    "\"\"\"Compute candidate 1-itemset\"\"\"\n",
    "C1 = {}\n",
    "\"\"\"total number of transactions contained in the file\"\"\"\n",
    "transactions = 0\n",
    "with open(\"NISTraffic.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        T = []\n",
    "        transactions += 1  \n",
    "        if transactions < 40:\n",
    "            for word in line.split(\",\"):\n",
    "                if word != '' and word != '\\n':\n",
    "                    T.append(word)\n",
    "                    if word not in C1.keys():\n",
    "                        C1[word] = 1\n",
    "                    else:\n",
    "                        count = C1[word]\n",
    "                        C1[word] = count + 1\n",
    "            D.append(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import pandas as pd\n",
    "\n",
    "print(len(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(D).transform(D)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "frequent_itemsets = apriori(df, min_support=0.01, use_colnames=False)\n",
    "\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minsup = minconf = 0.90\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max = int(len(D)/40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apriori = Apriori(D, minsup, minconf)\n",
    "apriori.run()\n",
    "apriori.print_frequent_itemset()\n",
    "apriori.print_rule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('NISTraffic.csv', sep = ' ')\n",
    "max = int(len(dataset)/16122)\n",
    "dataset = dataset.iloc[0:max,:]\n",
    "\n",
    "dataset = dataset.values.tolist()\n",
    "print(len(dataset))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "frequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)\n",
    "\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
    "\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
