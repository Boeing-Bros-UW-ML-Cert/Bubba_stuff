{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 10: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dataset(s) needed: MNIST (\"Modified National Institute of Standards and Technology\") dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Load the MNIST dataset\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Q.1. Split the data into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = mnist.data[:60000]\n",
    "test = mnist.data[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = mnist.target[:60000]\n",
    "test_y = mnist.target[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.2. Train a Logistic Regression classifier on the dataset and see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 15.60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "log_clf = LogisticRegression(solver='lbfgs',multi_class='multinomial').fit(train,train_y)\n",
    "end_time = time.time()\n",
    "\n",
    "og_duration = end_time - start_time\n",
    "\n",
    "print(\"Training took {:.2f}s\".format(og_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.3. Evaluate the resulting model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics:\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 963    0    0    3    1    3    4    4    2    0]\n",
      " [   0 1112    4    2    0    1    3    2   11    0]\n",
      " [   3   10  926   15    6    4   15    8   42    3]\n",
      " [   4    1   21  916    1   26    3    9   22    7]\n",
      " [   1    1    7    3  910    0    9    7   10   34]\n",
      " [  11    2    1   33   11  776   11    6   35    6]\n",
      " [   9    3    7    3    7   16  910    2    1    0]\n",
      " [   1    6   24    5    7    1    0  951    3   30]\n",
      " [   8    7    6   23    6   26   10   10  869    9]\n",
      " [   9    7    0   11   25    6    0   22    7  922]]\n",
      "\n",
      "Accuracy Score: 0.9255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "\n",
    "log_clf_preds = log_clf.predict(test)\n",
    "\n",
    "print('Performance metrics:\\n\\n')\n",
    "print ('Confusion Matrix:\\n',(confusion_matrix(test_y,log_clf_preds)))\n",
    "acc = accuracy_score(test_y,log_clf_preds)\n",
    "print('\\nAccuracy Score: {}'.format(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Q.4. Use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95, svd_solver='full')\n",
    "\n",
    "train_pca = pca.fit_transform(train)\n",
    "test_pca = pca.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Q.5. Train a new Logistic Regression classifier on the reduced dataset and see how long it takes. Was training much faster? Explain your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 7.27s, which is much shorter than the training time using the full dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca_clf = LogisticRegression(solver='lbfgs',multi_class='multinomial').fit(train_pca,train_y)\n",
    "end_time = time.time()\n",
    "\n",
    "pca_duration = end_time - start_time\n",
    "\n",
    "print(\"Training took {:.2f}s, which is much shorter than the training time using the full dataset\".format(pca_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.6. Evaluate the new classifier on the test set: how does it compare to the previous classifier? Discuss the speed / accuracy trade-off and in which case you'd prefer a very slight drop in model performance for a x-time speedup in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Score post-PCA: 0.9201\n",
      "\n",
      "The model using PCA for dimension reduction is clearly superior, when accounting for both training time and model performance. While the full dataset gives a slightly higher accuracy score, the training time is almost double.\n",
      "\n",
      "Whether or not this tradeoff is wise would depend on the usecase of the data. In a medical setting, any increase in accuracy is worth almost any computational cost.\n",
      "\n",
      "However, other lower-stakes settings might not be willing to pay the price in training costs. Particularly in big data settings, any reduction in training time is important.\n"
     ]
    }
   ],
   "source": [
    "pca_preds = pca_clf.predict(test_pca)\n",
    "\n",
    "acc = accuracy_score(test_y,pca_preds)\n",
    "print('\\nAccuracy Score post-PCA: {}'.format(acc))\n",
    "\n",
    "\n",
    "print(\"\\nThe model using PCA for dimension reduction is clearly superior, when accounting for both training time and model performance. While the full dataset gives a slightly higher accuracy score, the training time is almost double.\")\n",
    "print(\"\\nWhether or not this tradeoff is wise would depend on the usecase of the data. In a medical setting, any increase in accuracy is worth almost any computational cost.\")\n",
    "print(\"\\nHowever, other lower-stakes settings might not be willing to pay the price in training costs. Particularly in big data settings, any reduction in training time is important.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
